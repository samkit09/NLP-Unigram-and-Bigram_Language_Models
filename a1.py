# -*- coding: utf-8 -*-
"""A1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/188hz0gocoA4BMZxM92WCQ90KAEivaed7

### Part 1 - Computing unsmoothed unigram and bigram probabilities.
"""

# importing appropriate libraries
from collections import defaultdict
import os
import random
import math

# Reading Dataset
# corpus = open("/content/train.txt","r", encoding='utf-8').read()

# corpus.find("'business")

# corpus

# # Function that returns list of non-alphabetical words
# def get_non_alpha_words(words):
#     t = filter(lambda x: not x.isalpha() and len(x) > 0,words)
#     temp = []
#     for s in t:
#       if s not in temp:
#         temp.append(s)
#     return temp

# non_alpha = get_non_alpha_words(corpus.split(" "))
# print("Non-alphabetical character/words = ",len(non_alpha))
# # non_alpha

import re
def pre_process(ds):
    # defining character to replace
    # special_chars = ["\n", "-", '/', "..", "*"]
    # sent_ends = [" . ", " ? ", " ! "]

    ds = re.sub(r"wo n\'t", "will not", ds)
    ds = re.sub(r"ca n\'t", "can not", ds)

    # general
    ds = re.sub(r"n\'t", " not", ds)
    ds = re.sub(r"\'re", " are", ds)
    # ds = re.sub(r"\'s", "", ds)
    ds = re.sub(r"\'d", " would", ds)
    ds = re.sub(r"\'ll", " will", ds)
    ds = re.sub(r"\'t", " not", ds)
    ds = re.sub(r"\'ve", " have", ds)
    ds = re.sub(r"\'m", " am", ds)

    ds = re.sub('-', ' ', ds)
    # Combine special characters and sentence endings into a single pattern
    ds = re.sub('[^a-zA-Z0-9 \.!?]', '', ds)
    ds = ' '.join(ds.split())
    ds = ds.lower()

    # ds = re.sub(r'\.{2,}', '.', ds)
    # ds = re.sub(r'\?{2,}', '?', ds)
    # ds = re.sub(r'!{2,}', '!', ds)
    #  replacing special character and escape sequence characer with white space character


    ds = str('<s> ') + ds + str(' <e>')

    # lowering all the character to lowercase so the model don't take words like 'HELLO' and 'hello' as different words

    return ds

# preprocessing data
# ds = pre_process(corpus)
# tokens = ds.split(' ')

corpus_sent = []
with open("/content/train.txt", 'r') as file:
    for line in file:
        processed_line = pre_process(line)
        corpus_sent.append(processed_line)

corpus_sent[0]

# Block for counting words and storing in appropriate data structure

def get_counts(corpus_sent):
    vocab = []
    unigram = {}
    bigram = {}
    for sentence in corpus_sent:
      words = sentence.split(' ')
      for i in range(len(words) - 1):
        if words[i] not in vocab:
          vocab.append(words[i])

        unigram[words[i]] = unigram.get(words[i], 0) + 1

        if words[i] not in bigram:
          bigram[words[i]] = {}
        bigram[words[i]][words[i+1]] = bigram.get(words[i], {}).get(words[i + 1], 0) + 1

    if len(words) > 0 and words[-1] not in vocab:
      unigram[words[-1]] = unigram.get(words[-1], 0) + 1

    return vocab, unigram, bigram

# length of vocab
vocab, unigrams, bigrams = get_counts(corpus_sent)
V = len(vocab)
print(V)

def handle_unigram_unk(n = 2):
  unigram_unk = defaultdict(int)
  for k,v in unigrams.items():
    if v < n:
        unigram_unk['<UNK>'] = unigram_unk.get('<UNK>', 0) + v
    else:
        unigram_unk[k] = v
  return unigram_unk

def handle_bigram_unk(bigram, unigram, n=2):
    bi_gram_unk = defaultdict(lambda: defaultdict(int))

    for key, val in bigram.items():
        prev_word = '<UNK>' if unigram.get(key, 0) < n else key

        for k, v in val.items():
            if v < n:
                bi_gram_unk[prev_word]['<UNK>'] += v
            else:
                bi_gram_unk[prev_word][k] = v

    return bi_gram_unk

unigrams = handle_unigram_unk()
unigrams['i']

bigrams = handle_bigram_unk(bigrams, unigrams)
bigrams['<UNK>']

# declaring variables to store probabilities for unigram and bigram
unigram_probabilities = defaultdict(lambda:0)
bigram_probabilities = defaultdict(lambda:defaultdict(lambda:0))
N = sum(list(unigrams.values()))

N

# Loop to calculate and store unigram probabilities
for k,v in unigrams.items():
  unigram_probabilities[k] = unigrams[k]/N

unigram_probabilities['<s>']

# Loop to calculate and store bigram probabilities
bigram_probabilities = {}
for k,v in bigrams.items():
    if k not in bigram_probabilities:
      bigram_probabilities[k] = {}

    # n = sum(list(bigrams[k].values()))
    for k1,v1 in v.items():
        bigram_probabilities[k][k1] = bigrams[k][k1]/unigrams[k]

bigram_probabilities['<UNK>']

bigram_probabilities['<UNK>']['<UNK>']

"""## Part 2 - Applying Smoothing (Laplace and Add-k)

### Formula for Add-k smoothing:
![image.png](attachment:image.png)
"""

def get_prob_after_smoothing (counts, K = 1):
    '''
    counts: key value pairs with words as keys and count as values. Used to calculate probabilities for unigrams and bigrams
    '''
    try :
      _prob = defaultdict(lambda:K/N*K)
      for k,v in counts.items():
          # Formula for add-k smoothing
          _prob[k] = (v + K) / (N + (K*V))

    except:
      _prob = defaultdict(lambda:defaultdict(lambda:1/N))
      for k,v in counts.items():
        n = sum(list(v.values()))
#             print("n - ",n)
        temp = defaultdict(lambda:(K/(n+K*n)))
        for k1,v1 in v.items():
          try:
              # Formula for add-k smoothing
              temp[k1] = (v1+K)/(n+K*n)
          except:
              temp[k1] = 0
        _prob[k] = temp

    return _prob

"""### Laplace Smoothing"""

uni_gram_prob = get_prob_after_smoothing(unigrams)
bi_gram_prob = get_prob_after_smoothing(bigrams)

"""Note: In add-k smoothing, probability of all words usually decreases slightly as a small value (k) is added to the count of each word for unseen words and make sure that no word has 0 probability."""

# example of probabilities before and after smoothing in unigram
test_key = "two"
print("Without smoothing :\t",unigram_probabilities[test_key])
print("With smoothing : \t",uni_gram_prob[test_key])

bigram_probabilities['<UNK>']['<UNK>']

# example of probabilities before and after smoothing in bigram
given = "i"
find = "will"
if given not in bigram_probabilities.keys():
  given = '<UNK>'
  if find not in bigram_probabilities[given].keys():
    find = '<UNK>'

print("Without smoothing :\t",bigram_probabilities[given][find])
print("With smoothing : \t",bi_gram_prob[given][find])

"""### Add-K smoothing"""

uni_gram_k = get_prob_after_smoothing(unigrams,3)
bi_gram_k = get_prob_after_smoothing(bigrams,3)

# example of probabilities before and after smoothing in unigram
test_key = "two"
print("Without add-k smoothing :\t",unigram_probabilities[test_key])
print("With add-k smoothing : \t",uni_gram_k[test_key])

# example of probabilities before and after smoothing in bigram
given = "i"
find = "want"

if given not in bigram_probabilities.keys():
  given = '<UNK>'
  if find not in bigram_probabilities[given].keys():
    find = '<UNK>'
# print(given, find)
print("Without add-k smoothing :\t",bigram_probabilities[given][find])
print("With add-k smoothing : \t",bi_gram_k[given][find])

"""## Part 3 - Calculating Perplexity

![image.png](attachment:image.png)
"""

test_sent = []
with open("/content/val.txt", 'r') as file:
    for line in file:
        processed_line = pre_process(line)
        test_sent.append(processed_line)

test_corpus = ''
for i in range(len(test_sent)):
  if i == len(test_sent) - 1:
    test_corpus += test_sent[i]

  else:
    test_corpus += test_sent[i]
    test_corpus += ' '

import math

def get_number_of_unigrams(sentences):
    words = sentences.split(' ')
    return len(words)

def unigram_perplexity(sentences, probabilities):
    log_sum = 0
    N = get_number_of_unigrams(sentences)
    words = sentences.split(' ')
    for word in words:
      # print(word)
      if word in probabilities:
        try:
          log_sum += math.log2(probabilities[word])
        except:
          log_sum += math.log2(probabilities['<UNK>'])

    return math.pow(2, (-1 * (float(log_sum) / N)))


def bigram_perplexity(sentences, probabilities):
    N = get_number_of_unigrams(sentences)
    log_sum = 0
    words = sentences.split(' ')
    for i in range(1, len(words)):
        prev_word = words[i - 1]
        cur_word = words[i]
        if prev_word in probabilities:
          if cur_word in probabilities[prev_word]:
            try:
              log_sum += math.log2(probabilities[prev_word][cur_word])
            except:
              log_sum += math.log2(probabilities[prev_word]['<UNK>'])
        else:
          if cur_word in probabilities['<UNK>']:
            try:
              log_sum += math.log2(probabilities['<UNK>'][cur_word])
            except:
              log_sum += math.log2(probabilities['<UNK>']['<UNK>'])


    return math.pow(2, (-1 * (float(log_sum) / N)))


# Dummy function to get the number of unigrams

unigram_perplexity(test_corpus, unigram_probabilities)

bigram_perplexity(test_corpus, bigram_probabilities)

